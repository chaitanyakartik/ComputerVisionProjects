{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPl5JhwY/8dGuKDCoTtOYtu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":35,"metadata":{"id":"hwauwrdUvpt5","executionInfo":{"status":"ok","timestamp":1723273697483,"user_tz":-330,"elapsed":7,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}}},"outputs":[],"source":["#The steps:\n","#DO your imports\n","#get data and split into train test\n","#create the model\n","#define optimizer and loss function\n","#create the training loop"]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","from torch import nn\n","import numpy as np"],"metadata":{"id":"7E28vpBtwiHA","executionInfo":{"status":"ok","timestamp":1723273697483,"user_tz":-330,"elapsed":6,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["X = torch.arange(start=0, end=1, step=0.02).unsqueeze(dim=1)\n","weight = 0.7\n","bias = 0.3\n","y = weight*X + bias"],"metadata":{"id":"vkiT-IGUwi9f","executionInfo":{"status":"ok","timestamp":1723273697484,"user_tz":-330,"elapsed":7,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["train_split = int(0.8 * len(X))\n","X_train, y_train = X[:train_split], y[:train_split]\n","X_test, y_test = X[train_split:], y[train_split]"],"metadata":{"id":"RLRdTFs4xEHc","executionInfo":{"status":"ok","timestamp":1723273697484,"user_tz":-330,"elapsed":6,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["class LinearRegressionModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.weights = nn.Parameter(torch.randn(1,dtype=torch.float), requires_grad=True)\n","    self.bias = nn.Parameter(torch.randn(1,dtype=torch.float), requires_grad=True)\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","     return self.weights*x + self.bias"],"metadata":{"id":"NeYrQf5VxN5f","executionInfo":{"status":"ok","timestamp":1723273697484,"user_tz":-330,"elapsed":6,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["#Create the training loop\n","#Model in training mode\n","#Get Preds\n","#Calculate Loss\n","#Run Optimizer (1.set optim to zerograd 2.loss.backward 3.optimizer.step)\n","#Evaluate and print to keep track\n","\n","def trainingLoop(epochs,model,loss_fn,optimizer):\n","\n","  #Inference mode is used to make inferences (predictions). this is optimized for this\n","  with torch.inference_mode():\n","    y_preds = model(X_test)\n","\n","  # Create empty loss lists to track values\n","  train_loss_values = []\n","  test_loss_values = []\n","  epoch_count = []\n","\n","  for epoch in range(epochs):\n","\n","    #set model in train mode\n","    model.train()\n","\n","    #1.Forward step\n","    y_preds = model(X_train)\n","\n","    #2.Calculate loss\n","    loss = loss_fn(y_preds,y_train)\n","\n","    #3.Set optimizer grad zero\n","    optimizer.zero_grad()\n","\n","    #4.Backprop on loss\n","    loss.backward()\n","\n","    #5.Step forward on optimizer\n","    optimizer.step()\n","\n","    ### Testing\n","    # Put the model in evaluation mode\n","    model.eval()\n","\n","    with torch.inference_mode():\n","      # 1. Forward pass on test data\n","      test_pred = model(X_test)\n","\n","      # 2. Caculate loss on test data\n","      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n","\n","      # Print out what's happening\n","      if epoch % 10 == 0:\n","            epoch_count.append(epoch)\n","            train_loss_values.append(loss.detach().numpy())\n","            test_loss_values.append(test_loss.detach().numpy())\n","            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"],"metadata":{"id":"tvuNl4Sc2PTB","executionInfo":{"status":"ok","timestamp":1723274911596,"user_tz":-330,"elapsed":621,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# Defining our base model\n","model_0 = LinearRegressionModel()\n","# MAE loss is same as L1Loss\n","loss_function = nn.L1Loss()\n","# Create the optimizer\n","optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.01)\n","\n","trainingLoop(epochs=500,model=model_0,loss_fn=loss_function,optimizer=optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6xmy-V5JcGy","executionInfo":{"status":"ok","timestamp":1723274934015,"user_tz":-330,"elapsed":613,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}},"outputId":"15229eda-d643-4a90-d00d-79bf07dfbce2"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 | MAE Train Loss: 0.25122663378715515 | MAE Test Loss: 0.4075244963169098 \n","Epoch: 10 | MAE Train Loss: 0.14193940162658691 | MAE Test Loss: 0.2779034972190857 \n","Epoch: 20 | MAE Train Loss: 0.09095562994480133 | MAE Test Loss: 0.18873730301856995 \n","Epoch: 30 | MAE Train Loss: 0.07409852743148804 | MAE Test Loss: 0.13721208274364471 \n","Epoch: 40 | MAE Train Loss: 0.06757690757513046 | MAE Test Loss: 0.10811758041381836 \n","Epoch: 50 | MAE Train Loss: 0.06348922103643417 | MAE Test Loss: 0.09100444614887238 \n","Epoch: 60 | MAE Train Loss: 0.05999922752380371 | MAE Test Loss: 0.0799843817949295 \n","Epoch: 70 | MAE Train Loss: 0.05650923401117325 | MAE Test Loss: 0.06896430999040604 \n","Epoch: 80 | MAE Train Loss: 0.05305762216448784 | MAE Test Loss: 0.060004882514476776 \n","Epoch: 90 | MAE Train Loss: 0.04962732270359993 | MAE Test Loss: 0.05241922661662102 \n","Epoch: 100 | MAE Train Loss: 0.04619280621409416 | MAE Test Loss: 0.04414667561650276 \n","Epoch: 110 | MAE Train Loss: 0.04275400936603546 | MAE Test Loss: 0.03750298172235489 \n","Epoch: 120 | MAE Train Loss: 0.03932538628578186 | MAE Test Loss: 0.03316644951701164 \n","Epoch: 130 | MAE Train Loss: 0.03588919714093208 | MAE Test Loss: 0.02998087927699089 \n","Epoch: 140 | MAE Train Loss: 0.03245189040899277 | MAE Test Loss: 0.028413379564881325 \n","Epoch: 150 | MAE Train Loss: 0.02902345359325409 | MAE Test Loss: 0.027865642681717873 \n","Epoch: 160 | MAE Train Loss: 0.02558557130396366 | MAE Test Loss: 0.02872365154325962 \n","Epoch: 170 | MAE Train Loss: 0.022149959579110146 | MAE Test Loss: 0.03072839416563511 \n","Epoch: 180 | MAE Train Loss: 0.018720759078860283 | MAE Test Loss: 0.03354968875646591 \n","Epoch: 190 | MAE Train Loss: 0.015281951054930687 | MAE Test Loss: 0.03757941722869873 \n","Epoch: 200 | MAE Train Loss: 0.011848028749227524 | MAE Test Loss: 0.04206490516662598 \n","Epoch: 210 | MAE Train Loss: 0.008417141623795033 | MAE Test Loss: 0.047577548772096634 \n","Epoch: 220 | MAE Train Loss: 0.00497832428663969 | MAE Test Loss: 0.054087262600660324 \n","Epoch: 230 | MAE Train Loss: 0.0033503472805023193 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 240 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 250 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 260 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 270 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 280 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 290 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 300 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 310 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 320 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 330 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 340 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 350 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 360 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 370 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 380 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 390 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 400 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 410 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 420 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 430 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 440 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 450 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 460 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 470 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 480 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n","Epoch: 490 | MAE Train Loss: 0.004330190364271402 | MAE Test Loss: 0.06744243949651718 \n"]}]},{"cell_type":"code","source":["model_parameters = list(model_0.parameters())\n","\n","# Now 'model_parameters' is a list of parameter tensors\n","for param in model_parameters:\n","    print(param)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlCViR-RKcXT","executionInfo":{"status":"ok","timestamp":1723274945588,"user_tz":-330,"elapsed":614,"user":{"displayName":"Chaitanya","userId":"00175775561742849184"}},"outputId":"f7ab9cee-55b5-492f-dfff-2fd0e0127053"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([0.6906], requires_grad=True)\n","Parameter containing:\n","tensor([0.2993], requires_grad=True)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UpM2UpUUL5L4"},"execution_count":null,"outputs":[]}]}